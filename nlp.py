# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BpKLSkPAjEzEtLOo6BBdDb77VikHMgGU
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split,KFold
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
# %config InlineBackend.figure_format='retina'

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/full stack data science specialization/Module 4 (machine learning)/NLP/train.csv")

data.head()

data.info()

data['author'].value_counts().plot(kind='bar')

len(data['text'].iloc[0])

len(data['text'].iloc[0].split(" "))

def remove_punctuations(text):
  import string
  translator = str.maketrans('','',string.punctuation)
  return text.translate(translator)

data['text'] = data['text'].apply(remove_punctuations)

len(data['text'].iloc[0])

import nltk
nltk.download('stopwords')

sw = stopwords.words('english')
np.array(sw)

def remove_stopwords(text):
  text = [word.lower() for word in text.split() if word.lower() not in sw]
  return " ".join(text)

data['text'] = data['text'].apply(remove_stopwords)

len(data['text'].iloc[0])

data.head()

count_vectorizer = CountVectorizer()

count_vectorizer.fit(data['text'])
tuples = count_vectorizer.vocabulary_.items()

vocab = []
count = []
for key,value in tuples:
  vocab.append(key)
  count.append(value)

vocab_bef_stem = pd.Series(count,index=vocab)
vocab_bef_stem.sort_values(ascending=False).nlargest(10).plot(kind='barh',xlim= (25230, 25260))

stemmer = SnowballStemmer('english')
def stemming(text):
  text = [stemmer.stem(word) for word in text.split()]
  return " ".join(text)

data['text'] = data['text'].apply(stemming)

len(data['text'].iloc[0])

tfidf_vectorizer = TfidfVectorizer()

tfidf_vectorizer.fit(data['text'])
tuples = tfidf_vectorizer.vocabulary_.items()

vocab = []
count = []
for key,value in tuples:
  vocab.append(key)
  count.append(value)

pd.Series(count,index=vocab).sort_values(ascending=False).nlargest(20).plot(kind='barh',xlim=(15110,15150))

data['Length'] = data['text'].apply(lambda x : len(x))

data.head()

sns.histplot(data=data,x='Length',hue='author',bins=500)
plt.xlim(0,300)

array = tfidf_vectorizer.transform(data['text']).todense()
pd.DataFrame(array)